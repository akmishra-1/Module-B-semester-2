{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5190b54c-d49c-4d6f-80cc-22555336a9cd",
   "metadata": {},
   "source": [
    "# Week 2 - Preprocessing, part 2\n",
    "\n",
    "# 1. Lesson: None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4e5ff-b05f-4ef2-96f1-49dcb5beb158",
   "metadata": {},
   "source": [
    "# 2. Weekly graph question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad37e29-6e84-41fa-886d-abc1312213ab",
   "metadata": {},
   "source": [
    "The Storytelling With Data book mentions planning on a \"Who, What, and How\" for your data story.  Write down a possible Who, What, and How for your data, using the ideas in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7ee29",
   "metadata": {},
   "source": [
    "### Topic: Building Recommendation Systems for eCommerce\n",
    "### Project link:\n",
    "\n",
    "https://drive.google.com/file/d/1O5wSSYrw84Dpv2iaR5XzMz1ll0LgXBYK/view\n",
    "\n",
    "### Who\n",
    "\n",
    "The audience is the data science and product teams at a mid/large e-commerce or gaming platform \n",
    "companies like Amazon, Target, Olist, or Nintendo, Steam, or XBox. Specifically, I will target the \n",
    "lead data scientist who oversees recommendation algorithms, and a product manager responsible for personalization. \n",
    "These people are directly responsible for improving user satifaction with their platforms and conversion through\n",
    "better recommendations, so they would benefit most from exploratory insights that could uncover overlooked patterns \n",
    "or opportunities for better user data modelling.\n",
    "\n",
    "### What\n",
    "\n",
    "This project investigates how machine learning can enhance product recommendation systems in ecommerce by \n",
    "analysing real world data on user reivews, puchase behavior, product attributes, and sales. Using the following three\n",
    "datasets: Steam video game recommendations, Worldwide video game sales, and Olist retail data, I will explore the product categories\n",
    "and game genres that are most popular across different regions. I will also look at which features (ratings, timing, genre, price)\n",
    "correlate strongly with success measures like purchases or high review ratings. The goal is to uncover meaningful patterns that can inform\n",
    "how we model user preferences and improve personalization within online sales.\n",
    "\n",
    "### How\n",
    "\n",
    "I can deliver a structured report (slideument) or Jupyter notebook with clear annotated visuals, key takeaways, and a \n",
    "short conclusive summary of insights from the data. I'll compare how user interaction data, sales performance, and transactional behavior \n",
    "vary across the gaming and retail domains to answer the question: What product categories or game genres are most popular across regions? The intent is not to pitch a model yet, but to highlight promising trendsâ€”that could guide future\n",
    "recommendation system development and prioritize which features to engineer or model next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898eb327-aefd-4ac0-b95a-92b616a2181b",
   "metadata": {},
   "source": [
    "# 3. Homework - work with your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe925521-979f-4983-8d85-8db8d1316e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14836788-b235-4cd4-b94d-5f749c6141a8",
   "metadata": {},
   "source": [
    "This week, you will do the same types of exercises as last week, but you should use your chosen datasets that someone in your class found last semester. (They likely will not be the particular datasets that you found yourself.)\n",
    "\n",
    "### Here are some types of analysis you can do  Use Google, documentation, and ChatGPT to help you:\n",
    "\n",
    "- Summarize the datasets using info() and describe()\n",
    "\n",
    "- Are there any duplicate rows?\n",
    "\n",
    "- Are there any duplicate values in a given column (when this would be inappropriate?)\n",
    "\n",
    "- What are the mean, median, and mode of each column?\n",
    "\n",
    "- Are there any missing or null values?\n",
    "\n",
    "    - Do you want to fill in the missing value with a mean value?  A value of your choice?  Remove that row?\n",
    "\n",
    "- Identify any other inconsistent data (e.g. someone seems to be taking an action before they are born.)\n",
    "\n",
    "- Encode any categorical variables (e.g. with one-hot encoding.)\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "- Are the data usable?  If not, find some new data!\n",
    "\n",
    "- Do you need to modify or correct the data in some way?\n",
    "\n",
    "- Is there any class imbalance?  (Categories that have many more items than other categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to display mean, median, mode of a df\n",
    "def df_mmm_stats(df):\n",
    "    numeric_df = df.select_dtypes(include=\"number\")\n",
    "\n",
    "    means = numeric_df.mean()\n",
    "    medians = numeric_df.median()\n",
    "    modes = numeric_df.mode().iloc[0] # get da first mode, don\"t really care baout the others\n",
    "\n",
    "    summary_stats = pd.DataFrame({\"mean\": means, \"median\": medians,\"mode\": modes}).round(2) # two decimal places\n",
    "\n",
    "    return summary_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98153f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "olist_dataset_link = \"olistbr/brazilian-ecommerce\"\n",
    "vgrecs_dataset_link = \"antonkozyriev/game-recommendations-on-steam\"\n",
    "vgsales_dataset_link = \"ashaheedq/video-games-sales-2019\"\n",
    "\n",
    "olist_path = kagglehub.dataset_download(olist_dataset_link)\n",
    "vgrec_path = kagglehub.dataset_download(vgrecs_dataset_link)\n",
    "vgsales_path = kagglehub.dataset_download(vgsales_dataset_link)\n",
    "\n",
    "print(olist_path)\n",
    "print(vgrec_path)\n",
    "print(vgsales_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5b8da",
   "metadata": {},
   "source": [
    "## Loading and Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b156059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Game Sales DFs\n",
    "vgsales_df_raw = pd.read_csv(os.path.join(vgsales_path, \"vgsales-12-4-2019-short.csv\"))\n",
    "\n",
    "# Video Game Recommendation DFs\n",
    "vgrecs_recs_df_raw = pd.read_csv(os.path.join(vgrec_path, \"recommendations.csv\")) # merge with users based on \"user_id\" into vgrecs_merged_df_raw, add \"users_\" prefix to users.csv columns\n",
    "vgrecs_users_df_raw = pd.read_csv(os.path.join(vgrec_path, \"users.csv\"))\n",
    "vgrecs_games_df_raw = pd.read_csv(os.path.join(vgrec_path, \"games.csv\")) # merge with vgrecs_merged_df_raw on \"app_id\"\n",
    "\n",
    "vgrecs_merged_df = vgrecs_recs_df_raw.merge(vgrecs_users_df_raw, on=\"user_id\", how=\"left\")\n",
    "vgrecs_merged_df = vgrecs_merged_df.merge(vgrecs_games_df_raw, on=\"app_id\", how=\"left\")\n",
    "\n",
    "# Olist DFs\n",
    "olist_products_df_raw = pd.read_csv(os.path.join(olist_path, \"olist_products_dataset.csv\"))\n",
    "olist_translation_df_raw = pd.read_csv(os.path.join(olist_path, \"product_category_name_translation.csv\"))\n",
    "olist_reviews_df_raw = pd.read_csv(os.path.join(olist_path, \"olist_order_reviews_dataset.csv\"))\n",
    "olist_orders_df_raw = pd.read_csv(os.path.join(olist_path, \"olist_orders_dataset.csv\"))\n",
    "olist_geolocation_df_raw = pd.read_csv(os.path.join(olist_path, \"olist_geolocation_dataset.csv\"))\n",
    "olist_customers_df_raw = pd.read_csv(os.path.join(olist_path, \"olist_customers_dataset.csv\"))\n",
    "olist_items_df_raw = pd.read_csv(os.path.join(olist_path, \"olist_order_items_dataset.csv\"))\n",
    "olist_payments_df_raw = pd.read_csv(os.path.join(olist_path, \"olist_order_payments_dataset.csv\"))\n",
    "olist_sellers_df_raw = pd.read_csv(os.path.join(olist_path, \"olist_sellers_dataset.csv\"))\n",
    "\n",
    "# Merge the product categories with their english translations\n",
    "olist_products_df = (olist_products_df_raw.merge(olist_translation_df_raw, how=\"left\", on=\"product_category_name\"))\n",
    "# Merge the products with the order items so we know what the orders contained\n",
    "olist_order_lines_df = (olist_items_df_raw.merge(olist_products_df, how=\"left\", on=\"product_id\"))\n",
    "# Merge the orders with the orders\" customer ID, date, and order ID so we can merge it with the customer locations later\n",
    "olist_order_lines_df = olist_order_lines_df.merge(olist_orders_df_raw[[\"order_id\", \"customer_id\", \"order_purchase_timestamp\"]], how=\"left\", on=\"order_id\")\n",
    "# Merge the orders with the customer data\n",
    "olist_order_lines_df = olist_order_lines_df.merge(olist_customers_df_raw[[\"customer_id\", \"customer_unique_id\", \"customer_state\"]],how=\"left\",on=\"customer_id\")\n",
    "olist_merged_df = olist_order_lines_df.merge(olist_reviews_df_raw[[\"order_id\", \"review_score\", \"review_creation_date\", \"review_answer_timestamp\"]], how=\"left\", on=\"order_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede84c0b",
   "metadata": {},
   "source": [
    "# Processing the Video Game Sales Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3c8c9",
   "metadata": {},
   "source": [
    "## Features (from kaggle)\n",
    "| Column Name    | Description                                             |\n",
    "|----------------|---------------------------------------------------------|\n",
    "| Rank           | Ranking of overall sales                                |\n",
    "| Name           | Name of the game                                        |\n",
    "| Platform       | Platform of the game (i.e. PC, PS4, XOne, etc.)         |\n",
    "| Genre          | Genre of the game                                       |\n",
    "| ESRB Rating    | ESRB Rating of the game                                 |\n",
    "| Publisher      | Publisher of the game                                   |\n",
    "| Developer      | Developer of the game                                   |\n",
    "| Critic Score   | Critic score of the game out of 10                      |\n",
    "| User Score     | Users score the game out of 10                          |\n",
    "| Total Shipped  | Total shipped copies of the game                        |\n",
    "| Global_Sales   | Total worldwide sales (in millions)                     |\n",
    "| NA_Sales       | Sales in North America (in millions)                    |\n",
    "| PAL_Sales      | Sales in Europe (in millions)                           |\n",
    "| JP_Sales       | Sales in Japan (in millions)                            |\n",
    "| Other_Sales    | Sales in the rest of the world (in millions)            |\n",
    "| Year           | Year of release of the game                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f62a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats\n",
    "vgsales_df_raw.info()\n",
    "vgsales_df_raw.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e61061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the df\n",
    "vgsales_df_raw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686cc98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate rows\n",
    "print(\"Duplicate rows in vgsales:\", vgsales_df_raw.duplicated().sum())\n",
    "\n",
    "# No duplicates yeehaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean, median, mode\n",
    "df_mmm_stats(vgsales_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d2ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying nulls\n",
    "print(\"Video Game Sale Null Counts:\")\n",
    "print(vgsales_df_raw.isna().sum())\n",
    "print(vgsales_df_raw.shape)\n",
    "sales_scores_cols = [\n",
    "    \"Critic_Score\", \"User_Score\",\n",
    "    \"Total_Shipped\", \"Global_Sales\",\n",
    "    \"NA_Sales\", \"PAL_Sales\", \"JP_Sales\", \"Other_Sales\"\n",
    "]\n",
    "\n",
    "# Count rows where all of the performance column values (sales_scores_cols) are null\n",
    "all_null_sales_scores = vgsales_df_raw[sales_scores_cols].isna().all(axis=1).sum()\n",
    "print(\"Rows where all sales/scores are MISSING:\", all_null_sales_scores)\n",
    "\n",
    "\n",
    "# Of the 55792 rows in the Video Game Sale dataset, 32794 rows have missing performance metrics for\n",
    "# the whole row (e.g. scores, sales).This reduces the number of usable rows in the dataset (with at \n",
    "# least one performance metric) down to 55792 - 32794 = 22998 rows.\n",
    "# RANK CAN BE USED AS PERFORMANCE METRIC I GUESS\n",
    "\n",
    "\n",
    "# Count rows where all of the performance column values are present\n",
    "all_present_sales_scores = vgsales_df_raw[sales_scores_cols].notna().all(axis=1).sum()\n",
    "print(\"Rows with all sales/scores PRESENT:\", all_present_sales_scores)\n",
    "\n",
    "\n",
    "# Count rows where both Critic_Score and User_Score are NOT null\n",
    "both_scores_present = vgsales_df_raw[vgsales_df_raw[\"Critic_Score\"].notna() & vgsales_df_raw[\"User_Score\"].notna()].shape[0]\n",
    "print(\"Rows with BOTH Critic_Score and User_Score present:\", both_scores_present)\n",
    "\n",
    "\n",
    "# Drop rows with null \"Year\", should only be 17 rows discarded\n",
    "vgsales_df_cleaned = vgsales_df_raw.dropna(subset=[\"Year\"])\n",
    "\n",
    "# Fill null ESRB ratings with \"RP\" --> rating pending\n",
    "vgsales_df_cleaned[\"ESRB_Rating\"] = vgsales_df_cleaned[\"ESRB_Rating\"].fillna(\"RP\")\n",
    "\n",
    "# Null Developer values to \"Unknown\"\n",
    "vgsales_df_cleaned[\"Developer\"] = vgsales_df_cleaned[\"Developer\"].fillna(\"Unknown\")\n",
    "\n",
    "# Null sales values become 0:\n",
    "sales_cols = [\"Global_Sales\", \"NA_Sales\", \"PAL_Sales\", \"JP_Sales\", \"Other_Sales\"]\n",
    "vgsales_df_cleaned[sales_cols] = vgsales_df_cleaned[sales_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling inconsistencies\n",
    "\n",
    "# Drop rows for any years that are in the future (>2025) or past\n",
    "vgsales_df_cleaned = vgsales_df_cleaned[vgsales_df_cleaned[\"Year\"] <= 2025]\n",
    "\n",
    "\n",
    "# Ensure that the Global_Sales is not less than the sum of NA_Sales, PAL_Sales, JP_Sales, and Other_Sales\n",
    "regional_cols = [\"NA_Sales\", \"PAL_Sales\", \"JP_Sales\", \"Other_Sales\"]\n",
    "# sum the sales of the regions\n",
    "regional_sum = (vgsales_df_cleaned[\"NA_Sales\"] + vgsales_df_cleaned[\"PAL_Sales\"] + vgsales_df_cleaned[\"JP_Sales\"] + vgsales_df_cleaned[\"Other_Sales\"])\n",
    "# drop row if the global sales value is less thaen the regional sales value\n",
    "num_dropped_sales_rows = (vgsales_df_cleaned[\"Global_Sales\"] < regional_sum).sum()\n",
    "print(f\"Dropped {num_dropped_sales_rows} rows where the global sales value was less than the sum of sales in America, Europe, Japan, and other.\")\n",
    "vgsales_df_cleaned = vgsales_df_cleaned[vgsales_df_cleaned[\"Global_Sales\"] >= regional_sum]\n",
    "\n",
    "# Drop \"total shipped\" column, it has too many nulls and doesn't tell us anything helpful\n",
    "vgsales_df_cleaned = vgsales_df_cleaned.drop(\"Total_Shipped\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nulls again:\n",
    "print(\"Video Game Sale Null Counts:\")\n",
    "print(vgsales_df_cleaned.isna().sum())\n",
    "vgsales_df_cleaned.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the unique values to make sure they make sense\n",
    "categorical_cols = [\n",
    "    \"Platform\",\n",
    "    \"Genre\",\n",
    "    \"ESRB_Rating\",\n",
    "    \"Publisher\",\n",
    "    \"Developer\"\n",
    "]\n",
    "for col in categorical_cols:\n",
    "    unique_vals = vgsales_df_cleaned[col].unique()\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(f\"Number of unique values: {len(unique_vals)}\")\n",
    "    print(unique_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many times each type of platform so we can figue out how to encode it\n",
    "print(vgsales_df_cleaned[\"Platform\"].value_counts().to_string())\n",
    "\n",
    "# Probably top n encoding, up to 15 of the top platforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df with encoded categorical variables for modelling later\n",
    "vgsales_df_cleaned_encoded = vgsales_df_cleaned.copy()\n",
    "\n",
    "# One hot encode Genre and ESRB_Rating\n",
    "vgsales_df_cleaned_encoded = pd.get_dummies(vgsales_df_cleaned_encoded, columns=[\"Genre\", \"ESRB_Rating\"], prefix=[\"Genre\", \"ESRB\"])\n",
    "\n",
    "# Top n encode the platform\n",
    "top_platforms = vgsales_df_cleaned_encoded[\"Platform\"].value_counts().nlargest(10).index\n",
    "vgsales_df_cleaned_encoded[\"Platform\"] = vgsales_df_cleaned_encoded[\"Platform\"].where(vgsales_df_cleaned_encoded[\"Platform\"].isin(top_platforms), other=\"Other\")\n",
    "vgsales_df_cleaned_encoded = pd.get_dummies(vgsales_df_cleaned_encoded, columns=[\"Platform\"], prefix=\"Platform\")\n",
    "\n",
    "# Frequency encode publisher and developer\n",
    "for col in [\"Publisher\", \"Developer\"]:\n",
    "    freq_map = vgsales_df_cleaned_encoded[col].value_counts(normalize=True)\n",
    "    vgsales_df_cleaned_encoded[col + \"_freq\"] = vgsales_df_cleaned_encoded[col].map(freq_map)\n",
    "# Drop original columns\n",
    "vgsales_df_cleaned_encoded = vgsales_df_cleaned_encoded.drop(columns=[\"Publisher\", \"Developer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67702b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgsales_df_cleaned_encoded.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use this to create a df with no missing critic and user scores for better EDA later. I will keep all the other values in for now\n",
    "vgsales_eda_df = vgsales_df_cleaned.dropna(subset=[\"Critic_Score\", \"User_Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing there is a class imbalance inthe platform, since 20% are PC. but this isn't going to be a target, so it doesnt matter.\n",
    "vgsales_df_cleaned[\"Platform\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee124f9",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "### The data is usable and helpful. There are different success metrics such as rank, global sales, and some critics/user score. Rank is likely going to be the target variable for most models. The data was modified and corrected as needed, and a copy of the data was encoded for use later in models. There is a class imbalance for the Platform feature, where 20% of the data's games are made for PC. but we will be using this as a feature, not a target, so it is irrelevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd1d6a",
   "metadata": {},
   "source": [
    "# Processing the Video Game Recommendations Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f38752",
   "metadata": {},
   "source": [
    "## Features\n",
    "| Column Name     | Data Type  | Description                                                   |\n",
    "|----------------|------------|---------------------------------------------------------------|\n",
    "| app_id         | int64      | Unique game identifier on Steam                               |\n",
    "| helpful        | int64      | How many users found the recommendation helpful               |\n",
    "| funny          | int64      | How many users found the recommendation funny                 |\n",
    "| date           | object     | Date of publishing                                            |\n",
    "| is_recommended | bool       | Is the user recommending the product?                         |\n",
    "| hours          | float64    | How many hours played by the reviewing user                   |\n",
    "| user_id        | int64      | User's anonymized ID                                          |\n",
    "| review_id      | int64      | Autogenerated ID for the review itself                        |\n",
    "| products       | int64      | Number of games/add-ons purchased by the user                 |\n",
    "| reviews        | int64      | Number of reviews published by the reviewing user             |\n",
    "| title          | object     | Name of the game being reviewed                               |\n",
    "| date_release   | object     | Game's date of release                                        |\n",
    "| win            | bool       | Supports Windows OS                                           |\n",
    "| mac            | bool       | Supports Mac OS                                               |\n",
    "| linux          | bool       | Supports Linux OS                                             |\n",
    "| rating         | object     | Product rating category                                       |\n",
    "| positive_ratio | int64      | Ratio of positive feedbacks (%)                               |\n",
    "| user_reviews   | int64      | Number of user reviews available on the Steam page            |\n",
    "| price_final    | float64    | Price in US dollars ($) calculated after the discount         |\n",
    "| price_original | float64    | Price in US dollars ($) before the discount                   |\n",
    "| discount       | float64    | Discount percentage                                            |\n",
    "| steam_deck     | bool       | Supports the Steam Deck                                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgrecs_merged_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acknowledge date\n",
    "vgrecs_merged_df['date_release'] = pd.to_datetime(vgrecs_merged_df['date_release'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats\n",
    "vgrecs_merged_df.info()\n",
    "vgrecs_merged_df.describe(include=\"all\")\n",
    "\n",
    "# everything looks normal, all numerical columns are within acceptable ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec2cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking columns for duplicate values\n",
    "print(\"Duplicate rows in vgrecs:\", vgrecs_merged_df.duplicated().sum())\n",
    "\n",
    "# Check if there are any duplicated game names with the same app_id\n",
    "vgrecs_merged_df[vgrecs_merged_df.duplicated(subset=[\"app_id\", \"title\"], keep=False)]\n",
    "\n",
    "# No duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any duplicated reviews\n",
    "vgrecs_merged_df[vgrecs_merged_df.duplicated(\"review_id\", keep=False)]\n",
    "\n",
    "# None are duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean, median, mode\n",
    "df_mmm_stats(vgrecs_games_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Game Recommendation Null Counts:\n",
    "print(vgrecs_merged_df.isna().sum())\n",
    "print(vgrecs_merged_df.shape)\n",
    "\n",
    "# Perfect data, hallelujah\n",
    "# No values to impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59094fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify inconsistent/unusable data\n",
    "\n",
    "# check for any invalid dates\n",
    "print(\"Number of missing date_release:\", vgrecs_merged_df['date_release'].isna().sum())\n",
    "\n",
    "# verify unique values\n",
    "print(vgrecs_merged_df.shape[0])\n",
    "for col in [\"review_id\", \"user_id\", \"app_id\", \"date_release\", \"rating\"]:\n",
    "    unique_vals = vgrecs_merged_df[col].unique()\n",
    "    print(f\"{col}: {len(unique_vals)}\")\n",
    "\n",
    "# everything looks normal here too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16adfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgrecs_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3519faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables\n",
    "\n",
    "vgrecs_merged_df_encoded = vgrecs_merged_df.copy()\n",
    "\n",
    "# Ordinal encode the review rating\n",
    "# get unique values\n",
    "vgrecs_merged_df[\"rating\"].unique()\n",
    "# create a map to encode numbers to values\n",
    "rating_order = ['Overwhelmingly Negative', 'Very Negative', 'Mostly Negative', 'Negative', 'Mixed', 'Positive', 'Mostly Positive', 'Very Positive', 'Overwhelmingly Positive']\n",
    "rating_map = {label: idx for idx, label in enumerate(rating_order)}\n",
    "print(rating_map)\n",
    "# apply the mapping (low numbers are low ratings, high numbers are high ratings - easy to rememeber)\n",
    "vgrecs_merged_df_encoded['rating_encoded'] = vgrecs_merged_df_encoded['rating'].map(rating_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing there is a significant class imbalance in the rating values, since 50% are overwhelmingly positive.\n",
    "# This will be taken into consideration when modelling (balancing class weights)\n",
    "vgrecs_merged_df['rating'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed959c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgrecs_merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9cf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing there is also significant class imbalance in the is_recommended values, since 80% are True.\n",
    "# Because anything above a \"mixed\" rating will likely also be recommended, this imbalance can be taken \n",
    "# into consideration when generating the model by enabling the class weights parameter or resampling the data.\n",
    "vgrecs_merged_df[\"is_recommended\"].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde3833",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### The data is very clean and will be perfect for my use case. There are no null values, duplicates, or inconsistencies. None of the data needs to be modified in any way prior to usage, with the exception of needing to ordinally encode the review rating to a number. There is a very large class imbalance on the outputs, but we can but we can apply class weighting to ensure the model doesn't just learn the dominant class. We can also use ensemble methods like Random Forests or XGBoost, which handle imbalanced data better than naive models like regression or something.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c77aa3",
   "metadata": {},
   "source": [
    "# Processing the Olist eCommerce Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226abe32",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "| Column Name                   | Data Type | Description                                                                                      |\n",
    "|------------------------------|-----------|--------------------------------------------------------------------------------------------------|\n",
    "| order_id                     | object    | Unique ID of the order                                                                           |\n",
    "| order_item_id                | int64     | Sequential number identifying number of items in the same order                                  |\n",
    "| product_id                   | object    | Unique product identifier                                                                         |\n",
    "| seller_id                    | object    | Seller unique identifier                                                                          |\n",
    "| shipping_limit_date          | object    | Seller shipping limit date for handing the order to the logistic partner                         |\n",
    "| price                        | float64   | Item price                                                                                        |\n",
    "| freight_value                | float64   | Item freight value (split between items if multiple)                                             |\n",
    "| product_category_name        | object    | Root category of product (in Portuguese)                                                         |\n",
    "| product_name_lenght          | float64   | Number of characters in the product name                                                         |\n",
    "| product_description_lenght  | float64   | Number of characters in the product description                                                  |\n",
    "| product_photos_qty           | float64   | Number of published product photos                                                               |\n",
    "| product_weight_g             | float64   | Product weight in grams                                                                          |\n",
    "| product_length_cm            | float64   | Product length in centimeters                                                                    |\n",
    "| product_height_cm            | float64   | Product height in centimeters                                                                    |\n",
    "| product_width_cm             | float64   | Product width in centimeters                                                                     |\n",
    "| product_category_name_english| object    | Product category name in English                                                                 |\n",
    "| customer_id                  | object    | Key to the orders dataset; each order has a unique customer_id                                   |\n",
    "| order_purchase_timestamp     | object    | Purchase timestamp                                                                                |\n",
    "| review_creation_date         | object    | Date the satisfaction survey was sent to the customer                                            |\n",
    "| review_answer_timestamp      | object    | Timestamp of the customer's survey response                                                      |\n",
    "| customer_unique_id           | object    | Unique identifier of a customer                                                                  |\n",
    "| customer_state               | object    | Customer's state/province in Brazil (2-letter abbreviation)                                      |\n",
    "| review_score                 | float64   | Score from 1 to 5 given by the customer on the satisfaction survey                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78caf8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "olist_cleaned_df = olist_merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the df\n",
    "print(olist_cleaned_df.shape)\n",
    "olist_cleaned_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d28c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats\n",
    "olist_cleaned_df.info()\n",
    "olist_cleaned_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate rows\n",
    "print(\"Duplicates rows in olist:\", olist_cleaned_df.duplicated().sum())\n",
    "\n",
    "# Delete any duplicates:\n",
    "olist_cleaned_df = olist_cleaned_df.drop_duplicates(keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d110a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows that dont have a product_category_name\n",
    "before = len(olist_cleaned_df)\n",
    "olist_cleaned_df = olist_cleaned_df.dropna(subset=['product_category_name'])\n",
    "after = len(olist_cleaned_df)\n",
    "print(f\"Removed {before - after} rows without product_category_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "print(olist_cleaned_df.isna().sum())\n",
    "\n",
    "# looks like there are 24 missing english product category names, remove those\n",
    "olist_cleaned_df = olist_cleaned_df.dropna(subset=['product_category_name_english'])\n",
    "\n",
    "# remove the rows without a review score, that is the target\n",
    "olist_cleaned_df = olist_cleaned_df.dropna(subset=['review_score'])\n",
    "\n",
    "# One row has missing product weight/dimensions, remove that\n",
    "olist_cleaned_df = olist_cleaned_df.dropna(subset=['product_weight_g'])\n",
    "\n",
    "\n",
    "#check nulls again\n",
    "print()\n",
    "print(olist_cleaned_df.isna().sum())\n",
    "# all nulls removed, yeehaw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract review_creation time from order_purchase_timestamp to get time between purchase and survey sent\n",
    "\n",
    "# both columns should be datetime\n",
    "olist_cleaned_df['review_creation_date'] = pd.to_datetime(olist_cleaned_df['review_creation_date'])\n",
    "olist_cleaned_df['order_purchase_timestamp'] = pd.to_datetime(olist_cleaned_df['order_purchase_timestamp'])\n",
    "\n",
    "# Create a new column for the time difference in hours\n",
    "olist_cleaned_df['days_to_review'] = (olist_cleaned_df['review_creation_date'] - olist_cleaned_df['order_purchase_timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a28264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking inconsistensies\n",
    "\n",
    "# Olist: Checking for orders delivered before purchase (which doesn\"t make sense)\n",
    "olist_orders_df_raw[\"order_purchase_timestamp\"] = pd.to_datetime(olist_orders_df_raw[\"order_purchase_timestamp\"])\n",
    "olist_orders_df_raw[\"order_delivered_customer_date\"] = pd.to_datetime(olist_orders_df_raw[\"order_delivered_customer_date\"])\n",
    "olist_orders_df_raw[\n",
    "    olist_orders_df_raw[\"order_delivered_customer_date\"] < olist_orders_df_raw[\"order_purchase_timestamp\"]\n",
    "]\n",
    "\n",
    "# If we subtract order_purchase_timestamp from review_creation_date, it should only be a positive time delta. remove any negative ones:\n",
    "num_negative = (olist_cleaned_df['days_to_review'] < pd.Timedelta(0)).sum()\n",
    "print(f\"Number of rows with negative days_to_review: {num_negative}\")\n",
    "olist_cleaned_df = olist_cleaned_df[olist_cleaned_df['days_to_review'] >= pd.Timedelta(0)]\n",
    "\n",
    "# Product IDs shouldn\"t map to multiple categories\n",
    "product_counts = olist_cleaned_df.groupby(\"product_id\")[\"product_category_name\"].nunique()\n",
    "print(product_counts.value_counts())\n",
    "# no_category_id = product_counts[product_counts == 0].index\n",
    "\n",
    "# clean up the typos in column names:\n",
    "olist_cleaned_df = olist_cleaned_df.rename(columns={'product_name_lenght': 'product_name_length'})\n",
    "olist_cleaned_df = olist_cleaned_df.rename(columns={'product_description_lenght': 'product_description_length'})\n",
    "print(olist_cleaned_df.columns)\n",
    "\n",
    "# Drop product_category_name since we already have the english translation\n",
    "olist_cleaned_df = olist_cleaned_df.drop(\"product_category_name\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38741d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean, Median, Mode\n",
    "df_mmm_stats(olist_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding variables\n",
    "\n",
    "# verify unique values\n",
    "print(olist_cleaned_df.shape[0])\n",
    "for col in olist_cleaned_df.columns:\n",
    "    unique_vals = olist_cleaned_df[col].unique()\n",
    "    print(f\"{col}: {len(unique_vals)}\")\n",
    "\n",
    "olist_cleaned_df.info()\n",
    "\n",
    "\n",
    "# don't need ids for modelling \n",
    "olist_cleaned_encoded_df = olist_cleaned_df.copy()\n",
    "olist_cleaned_encoded_df = olist_cleaned_encoded_df.drop([\"order_id\", \"customer_id\", \"product_id\", \"seller_id\", \"customer_unique_id\"], axis=1)\n",
    "# important times are covered by days_to_review anyway, drop them\n",
    "olist_cleaned_encoded_df = olist_cleaned_encoded_df.drop([\"review_creation_date\", \"shipping_limit_date\", \"review_answer_timestamp\"], axis=1)\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "olist_cleaned_encoded_df = pd.get_dummies(olist_cleaned_encoded_df, columns=['product_category_name_english', 'customer_state'])\n",
    "\n",
    "\n",
    "# Convert timedelta to days (float)\n",
    "olist_cleaned_encoded_df['days_to_review'] = olist_cleaned_encoded_df['days_to_review'].dt.total_seconds() / 86400  # seconds in a day\n",
    "\n",
    "olist_cleaned_encoded_df.sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8218c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for review ratings class imbalance \n",
    "print(olist_cleaned_df['review_score'].value_counts(normalize=True))\n",
    "\n",
    "# yes, there is a class imbalance problem whereby 56% of the rows have a 5 star rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab384e2",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "### The data is usable, we have 110741 rows remaining from the original 113314 rows in the merged df. None of the values are null or inconsistent data. The olist timestamps were subtracted in order to get a column of time lapsed between the order purchase and the customer satisfaction survey. Additionally, the categorical variables like product category was encoded.Given the strong class imbalance in review_score, for classification models, I would either binarize the target (good vs. bad reviews), or use class weights to prevent misclassifying smaller classes. For regression models, I can retain the full score range and ensure evaluation metrics like RMSE or MAE to get prediction quality across all scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab9e6d-18cc-4863-b980-3e52f581763a",
   "metadata": {},
   "source": [
    "# 4. Storytelling With Data graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911148d-9df6-4b33-a875-8c96408ec834",
   "metadata": {},
   "source": [
    "Just like last week: choose any graph in the Introduction of Storytelling With Data. Use matplotlib to reproduce it in a rough way. I don't expect you to spend an enormous amount of time on this; I understand that you likely will not have time to re-create every feature of the graph. However, if you're excited about learning to use matplotlib, this is a good way to do that. You don't have to duplicate the exact values on the graph; just the same rough shape will be enough.  If you don't feel comfortable using matplotlib yet, do the best you can and write down what you tried or what Google searches you did to find the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2888f9-3700-45ab-9829-6a5372106f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake data similar to the line chart in storytelling with data page 1\n",
    "years = np.array([2010, 2011, 2012, 2013, 2014, 2015])\n",
    "data = {\n",
    "\"Arts & culture\":     [80, 25, 28, 40, 32, 45],\n",
    "\"Education\":          [20, 78, 45, 65, 63, 58],\n",
    "\"Health\":             [65, 52, 60, 63, 68, 93],\n",
    "\"Human services\":     [60, 85, 78, 60, 89, 55],\n",
    "\"Other\":              [30, 30, 45, 27, 47, 30]}\n",
    "\n",
    "# plot lines \n",
    "plt.figure(figsize=(8, 6))\n",
    "for label, values in data.items():\n",
    "    plt.plot(years, values, label=label)\n",
    "\n",
    "\n",
    "plt.title(\"Non Profit Support\")\n",
    "plt.ylabel(\"Percentage of support\")\n",
    "plt.xlabel(\"Year\")\n",
    "# ticks for the axes\n",
    "plt.xticks(years)\n",
    "plt.yticks(np.arange(0, 101, 10))\n",
    "plt.ylim(0, 100)\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
